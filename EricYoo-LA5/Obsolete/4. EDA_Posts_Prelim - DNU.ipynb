{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# NEWS = 0\n",
    "# UPNEWS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './API-data/ps_news_posts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-27b506e43c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./API-data/ps_news_posts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnews_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './API-data/ps_news_posts'"
     ]
    }
   ],
   "source": [
    "json_data=open('./API-data/ps_news_posts').read()\n",
    "news_posts = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data=open('./API-data/ps_upnews_posts').read()\n",
    "upnews_posts = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upnews_headlines = [li['title'] for li in upnews_posts]\n",
    "\n",
    "df_upnews = pd.DataFrame(upnews_headlines, columns=['headlines'])\n",
    "\n",
    "df_upnews.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headlines = [li['title'] for li in news_posts]\n",
    "\n",
    "df_news = pd.DataFrame(news_headlines, columns=['headlines'])\n",
    "\n",
    "df_news.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upnews['news'] = 1\n",
    "df_news['news'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df_upnews, df_news), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()\n",
    "results = []\n",
    "\n",
    "for line in news_headlines:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    pol_score['headline'] = line\n",
    "    results.append(pol_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.headline = df.headline.map(lambda x: re.sub('[^a-zA-Z0-9\\s]','',x))\n",
    "df.headline = df.headline.map(lambda x: re.sub('/r/News', ' ', x))\n",
    "df.headline = df.headline.map(lambda x: re.sub('/r/Upliftingnews', ' ', x))\n",
    "df.headline = df.headline.map(lambda x: re.sub('http[^\\s]*', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = 0\n",
    "df.loc[df['compound'] > 0.2, 'label'] = 1\n",
    "df.loc[df['compound'] < -0.2, 'label'] = -1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.label.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive headlines:\\n\")\n",
    "pprint(list(df[df['label'] == 1].headline)[:5], width=200)\n",
    "\n",
    "print(\"\\nNegative headlines:\\n\")\n",
    "pprint(list(df[df['label'] == -1].headline)[:5], width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "counts = df.label.value_counts(normalize=True) * 100\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
    "\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(headlines):\n",
    "    tokens = []\n",
    "    for line in headlines:\n",
    "        toks = tokenizer.tokenize(line)\n",
    "        toks = [t.lower() for t in toks if t.lower() not in stop_words]\n",
    "        tokens.extend(toks)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lines = list(df[df.label == 1].headline)\n",
    "\n",
    "pos_tokens = process_text(pos_lines)\n",
    "pos_freq = nltk.FreqDist(pos_tokens)\n",
    "\n",
    "pd.DataFrame(pos_freq.most_common(10), columns=('word', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lines = list(df[df.label == -1].headline)\n",
    "\n",
    "neg_tokens = process_text(neg_lines)\n",
    "neg_freq = nltk.FreqDist(neg_tokens)\n",
    "\n",
    "pd.DataFrame(neg_freq.most_common(10), columns=('word', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = [x[1] for x in pos_freq.most_common()]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(y_val)\n",
    "\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Distribution (Positive)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = []\n",
    "for i, k, z, t in zip(y_val[0::4], y_val[1::4], y_val[2::4], y_val[3::4]):\n",
    "    y_final.append(math.log(i + k + z + t))\n",
    "\n",
    "x_val = [math.log(i + 1) for i in range(len(y_final))]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.xlabel(\"Words (Log)\")\n",
    "plt.ylabel(\"Frequency (Log)\")\n",
    "plt.title(\"Word Frequency Distribution (Positive)\")\n",
    "plt.plot(x_val, y_final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = [x[1] for x in neg_freq.most_common()]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(y_val)\n",
    "\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Distribution (Negative)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = []\n",
    "for i, k, z in zip(y_val[0::3], y_val[1::3], y_val[2::3]):\n",
    "    if i + k + z == 0:\n",
    "        break\n",
    "    y_final.append(math.log(i + k + z))\n",
    "\n",
    "x_val = [math.log(i+1) for i in range(len(y_final))]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.xlabel(\"Words (Log)\")\n",
    "plt.ylabel(\"Frequency (Log)\")\n",
    "plt.title(\"Word Frequency Distribution (Negative)\")\n",
    "plt.plot(x_val, y_final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('reddit_headlines_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()\n",
    "results2 = []\n",
    "\n",
    "for line in upnews_headlines:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    pol_score['headline'] = line\n",
    "    results2.append(pol_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_records(results2)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['label'] = 0\n",
    "df2.loc[df2['compound'] > 0.2, 'label'] = 1\n",
    "df2.loc[df2['compound'] < -0.2, 'label'] = -1\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.headline = df2.headline.map(lambda x: re.sub('^a-zA-Z0-9','',x))\n",
    "df2.headline = df2.headline.map(lambda x: re.sub('/r/News', ' ', x))\n",
    "df2.headline = df2.headline.map(lambda x: re.sub('/r/Upliftingnews', ' ', x))\n",
    "df2.headline = df2.headline.map(lambda x: re.sub('http[^\\s]*', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts2 = df2.label.value_counts()\n",
    "print(counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive headlines:\\n\")\n",
    "pprint(list(df2[df2['label'] == 1].headline)[:5], width=200)\n",
    "\n",
    "print(\"\\nNegative headlines:\\n\")\n",
    "pprint(list(df2[df2['label'] == -1].headline)[:5], width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "counts2 = df2.label.value_counts(normalize=True) * 100\n",
    "\n",
    "sns.barplot(x=counts2.index, y=counts2, ax=ax)\n",
    "\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
