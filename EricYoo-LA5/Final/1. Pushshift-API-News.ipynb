{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling posts via the PushShift.io API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Github user had created a python wrapper for the API, PSAW\n",
    "\n",
    "https://github.com/dmarx/psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = api.search_submissions(limit=3000)\n",
    "results = list(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PushShift and psaw documentation, I tried to create a simple function to pull posts starting from a given date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "start_epoch=int(dt.datetime(2018, 7, 1).timestamp())\n",
    "\n",
    "news = list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit='news',\n",
    "                            filter=['url','author', 'title', 'subreddit', 'id'],\n",
    "                            limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "While reseraching PushShift, I found a more involved function that allowed for more customization, which I am shamelessly using due to being more effective than the simple function.\n",
    "\n",
    "Credit: https://www.reddit.com/r/pushshift/comments/89pxra/pushshift_api_with_large_amounts_of_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(sub=None, before=None, after=None, ids=None, getSubmissions=True, getComments=False):\n",
    "    suffix=''\n",
    "    searchType = 'submission'\n",
    "    if getComments or not getSubmissions:\n",
    "        searchType='comment'\n",
    "    if (before is not None):\n",
    "        suffix += '&before='+str(before)\n",
    "    if (after is not None):\n",
    "        suffix += '&after='+str(after)\n",
    "    if (sub is not None):\n",
    "        suffix += '&subreddit='+sub\n",
    "    if (ids is not None):\n",
    "        suffix += '&ids='+','.join(ids)\n",
    "\n",
    "    url = 'https://api.pushshift.io/reddit/search/'+searchType+'?sort=desc&size=1500'+suffix\n",
    "    print('loading '+url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.content)\n",
    "    if len(data['data']) > 0:\n",
    "        prev_end_date = data['data'][-1]['created_utc']\n",
    "    else:\n",
    "        prev_end_date = None\n",
    "    return (data, prev_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/submission?sort=desc&size=1500&after=60d&subreddit=news\n"
     ]
    }
   ],
   "source": [
    "sub='news'\n",
    "(submissions_tmp, prev_end_date) = getPushshiftData(sub=sub, after='60d')\n",
    "submissions = submissions_tmp['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/submission?sort=desc&size=1500&before=1536553451&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/submission?sort=desc&size=1500&before=1536525316&after=1d&subreddit=news\n"
     ]
    }
   ],
   "source": [
    "while prev_end_date is not None:\n",
    "    (submissions_tmp, prev_end_date) = getPushshiftData(sub=sub, before=prev_end_date-1, after='1d')\n",
    "    if prev_end_date is not None:\n",
    "        submissions.extend(submissions_tmp['data'])\n",
    "        \n",
    "# In the interest of not having 100 lines of \"loading\" below, I've modified this cell to only pull posts from 1 day.\n",
    "# The data used in this project was pulled from 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&after=60d&subreddit=news\n"
     ]
    }
   ],
   "source": [
    "(comments_tmp, prev_end_date) = getPushshiftData(sub=sub, after='60d', getComments=True)\n",
    "comments = comments_tmp['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536608355&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536604640&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536600030&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536595812&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536593690&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536591282&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536588154&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536583844&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536577575&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536567276&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536559614&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536554988&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536551853&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536549123&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536546625&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536543735&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536540051&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536535908&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536531931&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536528505&after=1d&subreddit=news\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536525260&after=1d&subreddit=news\n"
     ]
    }
   ],
   "source": [
    "while prev_end_date is not None:\n",
    "    (comments_tmp, prev_end_date) = getPushshiftData(sub=sub, before=prev_end_date-1, after='1d', getComments=True)\n",
    "    if prev_end_date is not None:\n",
    "        comments.extend(comments_tmp['data'])\n",
    "        \n",
    "# In the interest of not having 100 lines of \"loading\" below, I've modified this cell to only pull posts from 1 day.\n",
    "# The data used in this project was pulled from 60 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the raw posts and comments as json files, then repeat the same process for the next subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ps_news_1', 'w+') as f:\n",
    "    json.dump(comments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ps_news_posts_1', 'w+') as f:\n",
    "    json.dump(submissions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/submission?sort=desc&size=1500&after=1d&subreddit=upliftingnews\n",
      "loading https://api.pushshift.io/reddit/search/submission?sort=desc&size=1500&before=1536527064&after=1d&subreddit=upliftingnews\n"
     ]
    }
   ],
   "source": [
    "sub='upliftingnews'\n",
    "(submissions_tmp, prev_end_date) = getPushshiftData(sub=sub, after='1d')\n",
    "submissions_up = submissions_tmp['data']\n",
    "while prev_end_date is not None:\n",
    "    (submissions_tmp, prev_end_date) = getPushshiftData(sub=sub, before=prev_end_date-1, after='1d')\n",
    "    if prev_end_date is not None:\n",
    "        submissions_up.extend(submissions_tmp['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&after=1d&subreddit=upliftingnews\n",
      "loading https://api.pushshift.io/reddit/search/comment?sort=desc&size=1500&before=1536525924&after=1d&subreddit=upliftingnews\n"
     ]
    }
   ],
   "source": [
    "(comments_tmp, prev_end_date) = getPushshiftData(sub=sub, after='1d', getComments=True)\n",
    "comments_up = comments_tmp['data']\n",
    "while prev_end_date is not None:\n",
    "    (comments_tmp, prev_end_date) = getPushshiftData(sub=sub, before=prev_end_date-1, after='1d', getComments=True)\n",
    "    if prev_end_date is not None:\n",
    "        comments_up.extend(comments_tmp['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ps_upnews_2', 'w+') as f:\n",
    "    json.dump(comments_up, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ps_upnews_posts_2', 'w+') as f:\n",
    "    json.dump(submissions_up, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the length of the data that was pulled, as well as checking for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20801"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([p['title'] for p in submissions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submissions_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([p['title'] for p in submissions_up]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "There are roughly 80k posts and 250k comments for r/news and only 7k posts and 180k comments for r/upliftingnews. This dataset will certainly be unbalanced if using the pulled data as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
