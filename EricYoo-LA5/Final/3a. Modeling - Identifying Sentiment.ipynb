{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model to identify between the news and uplifting news subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data and vectorizing it, I want to create and train a model with a high accuracy rate in order to use for predicting. I will want to test using multiple classification models (specifically naive bayes, logistic regression, and random forest). I will be using the two datasets - SVD and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the cleaned posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/svd_df.pkl', 'rb') as f:\n",
    "    svd_df = pickle.load(f)\n",
    "# with open('../pickle/target_sub.pkl', 'rb') as f:\n",
    "#     target_sub = pickle.load(f)\n",
    "with open('../pickle/target_sent.pkl', 'rb') as f:\n",
    "    target_sent = pickle.load(f)\n",
    "with open('../pickle/term_df.pkl', 'rb') as f:\n",
    "    term_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD values are always non-negative, so I'll have to get the absolute values of the whole dataset; otherwise, I'll encounter an error down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = abs(svd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "I will be testing three models to compare against each other and use the best scoring model for my predictor. The models are:\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(svd_df, target_sent, stratify=target_sent, random_state=42)\n",
    "X_train_term, X_test_term, y_train_term, y_test_term = train_test_split(term_df, target_sent, stratify=target_sent, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "Naive Bayes Classifiers require the assumption that all features are independent which may not make this model the best fit for sentences or contextual words. However, cleaning the text (stop words) and adding weights (TF-IDF) will support this assumption. \n",
    "\n",
    "This model is a fairly simple one, so the only parameter to be tuned for will be the alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gs = GridSearchCV(MultinomialNB(), {'alpha': (1.0, 0.95, .9)})\n",
    "nb_term_gs = GridSearchCV(MultinomialNB(), {'alpha': (1.0, 0.95, .9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': (1.0, 0.95, 0.9)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.fit(X_train, y_train)\n",
    "nb_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.563831988412994, 0.8547486033519553)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_score_, nb_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'alpha': 0.9}, {'alpha': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_params_, nb_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_estimator_, nb_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that the SVD vectorized dataset performed better with a 0.9 alpha value, while the TF-IDF'ed model used 1.0. The alpha is a smoothing factor wherein a value is included to reduce the amount of 0 probabilities - perhaps the words in the SVD dataset are able to be classified with more certainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.56342, std: 0.00260, params: {'alpha': 1.0},\n",
       "  mean: 0.56363, std: 0.00232, params: {'alpha': 0.95},\n",
       "  mean: 0.56383, std: 0.00240, params: {'alpha': 0.9}],\n",
       " [mean: 0.85475, std: 0.00415, params: {'alpha': 1.0},\n",
       "  mean: 0.85454, std: 0.00437, params: {'alpha': 0.95},\n",
       "  mean: 0.85371, std: 0.00558, params: {'alpha': 0.9}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.grid_scores_, nb_term_gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5663149182702255, 0.5592799503414029)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.score(X_train, y_train), nb_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9265466583902338, 0.8497827436374923)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_term_gs.score(X_train_term, y_train_term), nb_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "The model scored terribly using the SVD data, and did significantly better using the TF-IDF data. Both models are overfitting based on the training data, though. Next step is to try Logistic Regression to see if it fares any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41853322, 0.58146678],\n",
       "       [0.49963369, 0.50036631],\n",
       "       [0.47300985, 0.52699015],\n",
       "       ...,\n",
       "       [0.40231758, 0.59768242],\n",
       "       [0.41484672, 0.58515328],\n",
       "       [0.32321828, 0.67678172]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is useful for binary target datasets, as it models the probability of the default class (in this case, for positive sentiment). \n",
    "\n",
    "I will be tuning the model based on penalty, the C value, and the tolerance. I want to test the model with both L1 and L2 regularization, as well as the regularization strength (represented as C, the inverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'],\n",
    "                                            'C': np.logspace(.01, 1, 15), \n",
    "                                            'tol': (0.0001, 0.001, 0.01)})\n",
    "lr_term_gs = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'],\n",
    "                                                 'C': np.logspace(.01, 1, 15), \n",
    "                                                 'tol': (0.0001, 0.001, 0.01)})                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([ 1.02329,  1.20424,  1.41719,  1.6678 ,  1.96271,  2.30978,\n",
       "        2.71823,  3.1989 ,  3.76456,  4.43025,  5.21366,  6.1356 ,\n",
       "        7.22057,  8.49739, 10.     ]), 'tol': (0.0001, 0.001, 0.01)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.fit(X_train, y_train), \n",
    "lr_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7330850403476101, 0.8704738257810883)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_score_, lr_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 5.21366181472805, 'penalty': 'l2', 'tol': 0.0001},\n",
       " {'C': 4.430253439574549, 'penalty': 'l2', 'tol': 0.01})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_, lr_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=5.21366181472805, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " LogisticRegression(C=4.430253439574549, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.01, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_estimator_, lr_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models used L2 regularization (ridge), and the tol is higher (0.01) on the TF-IDF data, implying that there is less regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.70764, std: 0.01351, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.0001},\n",
       "  mean: 0.70805, std: 0.01390, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.001},\n",
       "  mean: 0.70908, std: 0.01258, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.01},\n",
       "  mean: 0.72357, std: 0.00600, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.0001},\n",
       "  mean: 0.72357, std: 0.00600, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.001}],\n",
       " [mean: 0.81730, std: 0.00844, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.0001},\n",
       "  mean: 0.81730, std: 0.00844, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.001},\n",
       "  mean: 0.81730, std: 0.00844, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.01},\n",
       "  mean: 0.86427, std: 0.00600, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.0001},\n",
       "  mean: 0.86427, std: 0.00600, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.001}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid = lr_gs.grid_scores_\n",
    "lr_term_grid = lr_term_gs.grid_scores_\n",
    "\n",
    "lr_grid[:5], lr_term_grid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8268156424581006, 0.7461204220980757)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.score(X_train, y_train), lr_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9770328988206083, 0.8758535071384234)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.score(X_train_term, y_train_term), lr_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_term_gs.predict(X_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[618 104]\n",
      " [ 96 793]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.87      0.86      0.86       722\n",
      "          1       0.88      0.89      0.89       889\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.59%\n",
      "\n",
      "F1 Score: 88.80\n",
      "\n",
      "COnfusion Matrix:\n",
      " [[618 104]\n",
      " [ 96 793]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, y_pred) * 100))\n",
    "print(\"\\nCOnfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "Again, both models appear prone to overfitting. However, both models scored much better than the Multinomial model.\n",
    "\n",
    "I will want to test this data in an ensemble method next - Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is a tree-based model, so it will be making a series of splits/decisions on features \"behind the scenes\" in order to output the best score. This model will vote on the best model after creating trees using a random sample of features and random sample of variables. \n",
    "\n",
    "This model has a lot of hyperparameters for tuning; I will be using the following:\n",
    "\n",
    "- n_estimators - decide how many trees should be tested for in the forest. \n",
    "- max_depth - having the default max depth is likely to contribute to overfitting.\n",
    "- criterion - decide whether the splits should be made based on gini or entropy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [10, 50, 100], \n",
    "                                                                'max_depth': (None, 2, 5), \n",
    "                                                                'criterion': ['gini', 'entropy']})\n",
    "rfc_term_gs = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [10, 50, 100], \n",
    "                                                                     'max_depth': (None, 2, 5), \n",
    "                                                                     'criterion': ['gini', 'entropy']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 50, 100], 'max_depth': (None, 2, 5), 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.fit(X_train, y_train)\n",
    "rfc_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6828057107386716, 0.8384026484585144)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_score_, rfc_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'criterion': 'entropy', 'max_depth': None, 'n_estimators': 100},\n",
       " {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 100})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_params_, rfc_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=3, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=3, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_estimator_, rfc_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuprisingly, the best parameters were for no max depth limit and for the most number of estimators - this model will likely be overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.60687, std: 0.01118, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 10},\n",
       "  mean: 0.65777, std: 0.00345, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 50},\n",
       "  mean: 0.67577, std: 0.00833, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100},\n",
       "  mean: 0.57294, std: 0.00686, params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 10},\n",
       "  mean: 0.55390, std: 0.00155, params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 50}],\n",
       " [mean: 0.81233, std: 0.00310, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 10},\n",
       "  mean: 0.82351, std: 0.00205, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 50},\n",
       "  mean: 0.82516, std: 0.00326, params: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100},\n",
       "  mean: 0.58783, std: 0.00442, params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 10},\n",
       "  mean: 0.55514, std: 0.00205, params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 50}])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid = rfc_gs.grid_scores_\n",
    "rfc_term_grid = rfc_term_gs.grid_scores_\n",
    "\n",
    "rfc_grid[:5], rfc_term_grid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9977239809642044, 0.6778398510242085)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.score(X_train, y_train), rfc_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9393751293192634, 0.8336436995654872)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_term_gs.score(X_train_term, y_train_term), rfc_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "As predicted, this model grossly overfits. I may be able to achieve a higher score with further tuning (i.e. higher tree count), but that the time for fitting and testing will only get longer and longer. \n",
    "\n",
    "As it stands, the current best model is Logistic Regression using the TF-IDF'd dataset. This outcome makes sense considering the lack of cleanup on the feature selection (which the Logistic Regression model has included as a paramter), and the lack of context to the words; though this may be helped by including a higher n-gram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with new data\n",
    "\n",
    "I will now want to test the model against new data to see if it can predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/tfidf.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_news = [\"OxyContin creator being sued for \\'significant role in causing opioid epidemic'\",\n",
    "            \"Serena Williams fined $17,000 for U.S. Open violations\",\n",
    "            \"Girl wins homecoming queen, then goes on to kick extra point in overtime to win football game.\",\n",
    "            \"Hundreds are still trapped from Florence\\'s flooding, and \\'the worst is still yet to come\",\n",
    "            \"Layoffs hit, prices lag as tariff pinches lobster industry\"]\n",
    "\n",
    "new_upnews = [\"The Sniping Scientists Whose Work Saved Millions of Lives\",\n",
    "            \"NICU volunteer donates a million dollars to local baby unit\",\n",
    "            \"He spent 27 years wrongly convicted of murder. He wants to spend the rest encouraging inmates to read\",\n",
    "            \"She heard their cries and couldn’t walk away, so she helped save 18 dogs in Kinston\",\n",
    "            \"San Antonio police credit four good Samaritans with pulling a driver out of his burning truck on U.S. 281 and he walked away from the crash unscathed.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_news = tfidf.transform(new_news)\n",
    "term_upnews = tfidf.transform(new_upnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26532039, 0.73467961],\n",
       "       [0.2267944 , 0.7732056 ],\n",
       "       [0.10926029, 0.89073971],\n",
       "       [0.94882055, 0.05117945],\n",
       "       [0.46478371, 0.53521629]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict_proba(term_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23437802, 0.76562198],\n",
       "       [0.19105892, 0.80894108],\n",
       "       [0.76819741, 0.23180259],\n",
       "       [0.12129401, 0.87870599],\n",
       "       [0.74987998, 0.25012002]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict_proba(term_upnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, -1,  1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict(term_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1, -1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict(term_upnews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Looking at the headlines and the sentiment being ascribed to them, it is clear that the model is not very accurate. \n",
    "\n",
    "For this particular purpose of sorting by sentiment, I would not want to rely on the current model for any sort of production. I believe more improvements have to be made on the actual sentiment classification rather than the model itself; because the sentiment analysis library/function (VADER) was not optimized, the model is predicting correctly on the wrong sentiment. \n",
    "\n",
    "For the largest improvements, I would want to spend more time on the preprocessing portion to try and select more relevant features, as well as improve the sentiment analysis (perhaps by increasing n-grams). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
