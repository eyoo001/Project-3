{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model to identify between the news and uplifting news subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data and vectorizing it, I want to create and train a model with a high accuracy rate in order to use for predicting. I will want to test using multiple classification models (specifically naive bayes, logistic regression, and random forest). I will be using the two datasets - SVD and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the cleaned posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/svd_df.pkl', 'rb') as f:\n",
    "    svd_df = pickle.load(f)\n",
    "with open('../pickle/target_sub.pkl', 'rb') as f:\n",
    "    target_sub = pickle.load(f)\n",
    "# with open('../pickle/target_sent.pkl', 'rb') as f:\n",
    "#     target_sent = pickle.load(f)\n",
    "with open('../pickle/term_df.pkl', 'rb') as f:\n",
    "    term_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD values are always non-negative, so I'll have to get the absolute values of the whole dataset; otherwise, I'll encounter an error down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = abs(svd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "I will be testing three models to compare against each other and use the best scoring model for my predictor. The models are:\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(svd_df, target_sub, stratify=target_sub, random_state=42)\n",
    "X_train_term, X_test_term, y_train_term, y_test_term = train_test_split(term_df, target_sub, stratify=target_sub, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "Naive Bayes Classifiers require the assumption that all features are independent which may not make this model the best fit for sentences or contextual words. However, cleaning the text (stop words) and adding weights (TF-IDF) will support this assumption. \n",
    "\n",
    "This model is a fairly simple one, so the only parameter to be tuned for will be the alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_gs = GridSearchCV(MultinomialNB(), {'alpha': (1.0, 0.95, .9)})\n",
    "nb_term_gs = GridSearchCV(MultinomialNB(), {'alpha': (1.0, 0.95, .9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': (1.0, 0.95, 0.9)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.fit(X_train, y_train)\n",
    "nb_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.574591351127664, 0.776536312849162)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_score_, nb_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'alpha': 1.0}, {'alpha': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_params_, nb_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.best_estimator_, nb_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57459, std: 0.00192, params: {'alpha': 1.0},\n",
       "  mean: 0.57438, std: 0.00163, params: {'alpha': 0.95},\n",
       "  mean: 0.57438, std: 0.00163, params: {'alpha': 0.9}],\n",
       " [mean: 0.77654, std: 0.00383, params: {'alpha': 1.0},\n",
       "  mean: 0.77612, std: 0.00460, params: {'alpha': 0.95},\n",
       "  mean: 0.77633, std: 0.00482, params: {'alpha': 0.9}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.grid_scores_, nb_term_gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5774881026277674, 0.5698324022346368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.score(X_train, y_train), nb_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8713014690668321, 0.7827436374922409)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_term_gs.score(X_train_term, y_train_term), nb_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5890489 , 0.4109511 ],\n",
       "       [0.54587878, 0.45412122],\n",
       "       [0.56220715, 0.43779285],\n",
       "       ...,\n",
       "       [0.53131735, 0.46868265],\n",
       "       [0.57786938, 0.42213062],\n",
       "       [0.46068086, 0.53931914]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "The model scored terribly using the SVD data, and did a bit better using the TF-IDF data. Both models are overfitting based on the training data, though. The classification decisions being made by the model also appear to be relatively vague, with the probabilities being close to 50/50 as opposed to clear-cut. This is likely due to the large overlap in words used between the two subreddits.\n",
    "\n",
    "Next step is to try Logistic Regression to see if it fares any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is useful for binary target datasets, as it models the probability of the default class (in this case, for the news subreddit). \n",
    "\n",
    "I will be tuning the model based on penalty, the C value, and the tolerance. I want to test the model with both L1 and L2 regularization, as well as the regularization strength (represented as C, the inverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'],\n",
    "                                            'C': np.logspace(.01, 1, 15), \n",
    "                                            'tol': (0.0001, 0.001, 0.01)})\n",
    "lr_term_gs = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'],\n",
    "                                                 'C': np.logspace(.01, 1, 15), \n",
    "                                                 'tol': (0.0001, 0.001, 0.01)})                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([ 1.02329,  1.20424,  1.41719,  1.6678 ,  1.96271,  2.30978,\n",
       "        2.71823,  3.1989 ,  3.76456,  4.43025,  5.21366,  6.1356 ,\n",
       "        7.22057,  8.49739, 10.     ]), 'tol': (0.0001, 0.001, 0.01)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.fit(X_train, y_train), \n",
    "lr_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7001862197392924, 0.776743223670598)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_score_, lr_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 3.198895109691398, 'penalty': 'l2', 'tol': 0.0001},\n",
       " {'C': 2.3097843187477425, 'penalty': 'l2', 'tol': 0.0001})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_, lr_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=3.198895109691398, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " LogisticRegression(C=2.3097843187477425, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_estimator_, lr_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.66418, std: 0.01098, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.0001},\n",
       "  mean: 0.66418, std: 0.00967, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.001},\n",
       "  mean: 0.66605, std: 0.00830, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.01},\n",
       "  mean: 0.68384, std: 0.00747, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.0001},\n",
       "  mean: 0.68384, std: 0.00747, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.001}],\n",
       " [mean: 0.73495, std: 0.00483, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.0001},\n",
       "  mean: 0.73474, std: 0.00468, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.001},\n",
       "  mean: 0.73433, std: 0.00439, params: {'C': 1.023292992280754, 'penalty': 'l1', 'tol': 0.01},\n",
       "  mean: 0.77447, std: 0.00688, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.0001},\n",
       "  mean: 0.77447, std: 0.00688, params: {'C': 1.023292992280754, 'penalty': 'l2', 'tol': 0.001}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid = lr_gs.grid_scores_\n",
    "lr_term_grid = lr_term_gs.grid_scores_\n",
    "\n",
    "lr_grid[:5], lr_term_grid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7808814400993171, 0.675356921166977)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.score(X_train, y_train), lr_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.904407200496586, 0.7846058348851644)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.score(X_train_term, y_train_term), lr_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67229373, 0.32770627],\n",
       "       [0.75040028, 0.24959972],\n",
       "       [0.37863694, 0.62136306],\n",
       "       ...,\n",
       "       [0.53893128, 0.46106872],\n",
       "       [0.614055  , 0.385945  ],\n",
       "       [0.2919485 , 0.7080515 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict_proba(X_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_term_gs.predict(X_test_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[736 139]\n",
      " [208 528]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.84      0.81       875\n",
      "          1       0.79      0.72      0.75       736\n",
      "\n",
      "avg / total       0.79      0.78      0.78      1611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.46%\n",
      "\n",
      "F1 Score: 75.27\n",
      "\n",
      "COnfusion Matrix:\n",
      " [[736 139]\n",
      " [208 528]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, y_pred) * 100))\n",
    "print(\"\\nCOnfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "Again, both models are very prone to overfitting. However, both models scored marginally better than the Multinomial model. The model works best using ridge regularization and a lower tolerance.\n",
    "\n",
    "I will want to test this data in an ensemble method next - Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is a tree-based model, so it will be making a series of splits/decisions on features \"behind the scenes\" in order to output the best score. This model will vote on the best model after creating trees using a random sample of features and random sample of variables. \n",
    "\n",
    "This model has a lot of hyperparameters for tuning; I will be using the following:\n",
    "\n",
    "- n_estimators - decide how many trees should be tested for in the forest. \n",
    "- max_depth - having the default max depth is likely to contribute to overfitting.\n",
    "- criterion - decide whether the splits should be made based on gini or entropy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [150, 170, 190],\n",
    "                                                                'max_depth': (10, 50, 100),\n",
    "                                                                'criterion': ['gini', 'entropy']})\n",
    "rfc_term_gs = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [150, 170, 190],\n",
    "                                                                     'max_depth': (10, 50, 100),\n",
    "                                                                     'criterion': ['gini', 'entropy']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [150, 170, 190], 'max_depth': (10, 50, 100), 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.fit(X_train, y_train)\n",
    "rfc_term_gs.fit(X_train_term, y_train_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6654252017380509, 0.7392923649906891)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_score_, rfc_term_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'criterion': 'gini', 'max_depth': 50, 'n_estimators': 190},\n",
       " {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 150})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_params_, rfc_term_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=190, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_estimator_, rfc_term_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.64577, std: 0.01307, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 150},\n",
       "  mean: 0.64701, std: 0.01106, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 170},\n",
       "  mean: 0.65136, std: 0.01064, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 190},\n",
       "  mean: 0.66211, std: 0.00768, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 150},\n",
       "  mean: 0.66274, std: 0.00413, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 170},\n",
       "  mean: 0.66543, std: 0.00134, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 190},\n",
       "  mean: 0.66211, std: 0.00768, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 150},\n",
       "  mean: 0.66274, std: 0.00413, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 170},\n",
       "  mean: 0.66543, std: 0.00134, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 190},\n",
       "  mean: 0.65508, std: 0.01322, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 150},\n",
       "  mean: 0.66005, std: 0.01042, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 170},\n",
       "  mean: 0.66191, std: 0.00747, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 190},\n",
       "  mean: 0.65839, std: 0.00205, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 150},\n",
       "  mean: 0.65694, std: 0.00471, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 170},\n",
       "  mean: 0.65984, std: 0.00665, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 190},\n",
       "  mean: 0.65839, std: 0.00205, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 150},\n",
       "  mean: 0.65694, std: 0.00471, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 170},\n",
       "  mean: 0.65984, std: 0.00665, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 190}],\n",
       " [mean: 0.65011, std: 0.00384, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 150},\n",
       "  mean: 0.64887, std: 0.00345, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 170},\n",
       "  mean: 0.64887, std: 0.00387, params: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 190},\n",
       "  mean: 0.72439, std: 0.00183, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 150},\n",
       "  mean: 0.72377, std: 0.00308, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 170},\n",
       "  mean: 0.72646, std: 0.00337, params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 190},\n",
       "  mean: 0.73929, std: 0.00396, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 150},\n",
       "  mean: 0.73929, std: 0.00396, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 170},\n",
       "  mean: 0.73909, std: 0.00192, params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 190},\n",
       "  mean: 0.64308, std: 0.00134, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 150},\n",
       "  mean: 0.64267, std: 0.00293, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 170},\n",
       "  mean: 0.64287, std: 0.00279, params: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 190},\n",
       "  mean: 0.71695, std: 0.00659, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 150},\n",
       "  mean: 0.71860, std: 0.00425, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 170},\n",
       "  mean: 0.71964, std: 0.00547, params: {'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 190},\n",
       "  mean: 0.73784, std: 0.00642, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 150},\n",
       "  mean: 0.73888, std: 0.00425, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 170},\n",
       "  mean: 0.73929, std: 0.00571, params: {'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 190}])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.grid_scores_, rfc_term_gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9952410511069729, 0.6641837368094351)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.score(X_train, y_train), rfc_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8698530933167805, 0.7523277467411545)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_term_gs.score(X_train_term, y_train_term), rfc_term_gs.score(X_test_term, y_test_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "As predicted, this model grossly overfits. I may be able to achieve a higher score with further tuning (i.e. higher tree count), but that the time for fitting and testing will only get longer and longer. \n",
    "\n",
    "As it stands, the current best model is Logistic Regression using the TF-IDF'd dataset. This outcome makes sense considering the lack of cleanup on the feature selection (which the Logistic Regression model has included as a paramter), and the lack of context to the words; though this may be helped by including a higher n-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with new data\n",
    "\n",
    "I will now want to test the model against new data to see if it can predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/tfidf.pkl', 'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_news = [\"OxyContin creator being sued for \\'significant role in causing opioid epidemic'\",\n",
    "            \"Serena Williams fined $17,000 for U.S. Open violations\",\n",
    "            \"Girl wins homecoming queen, then goes on to kick extra point in overtime to win football game.\",\n",
    "            \"Hundreds are still trapped from Florence\\'s flooding, and \\'the worst is still yet to come\",\n",
    "            \"Layoffs hit, prices lag as tariff pinches lobster industry\"]\n",
    "\n",
    "new_upnews = [\"The Sniping Scientists Whose Work Saved Millions of Lives\",\n",
    "            \"NICU volunteer donates a million dollars to local baby unit\",\n",
    "            \"He spent 27 years wrongly convicted of murder. He wants to spend the rest encouraging inmates to read\",\n",
    "            \"She heard their cries and couldnâ€™t walk away, so she helped save 18 dogs in Kinston\",\n",
    "            \"San Antonio police credit four good Samaritans with pulling a driver out of his burning truck on U.S. 281 and he walked away from the crash unscathed.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_news = tfidf.transform(new_news)\n",
    "term_upnews = tfidf.transform(new_upnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64352352, 0.35647648],\n",
       "       [0.82392992, 0.17607008],\n",
       "       [0.49874612, 0.50125388],\n",
       "       [0.55841746, 0.44158254],\n",
       "       [0.78503572, 0.21496428]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict_proba(term_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05706687, 0.94293313],\n",
       "       [0.06150192, 0.93849808],\n",
       "       [0.59157234, 0.40842766],\n",
       "       [0.08219673, 0.91780327],\n",
       "       [0.43900884, 0.56099116]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict_proba(term_upnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict(term_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_term_gs.predict(term_upnews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The model was able to predict with 80% accuracy, which is better than I was anticipating considering the lower score and the performance of the sentiment model. This outcome also surprises me considering that the news subreddit was not as dominantly 'negative' in sentiment as I had expected, which is where I would have figured the main differences in words for subreddit would have come from. Though, I do still believe that a significant factor in this model being able to predict with some level of accuracy does come from the bias for negative news in r/news over r/upliftingnews.\n",
    "\n",
    "In terms of improvements to this model, they are mostly the same as with the sentiment model - better cleanup and vectorizing in the previous step, and utilizing more processing time/power to optimize the random forest classifier.\n",
    "\n",
    "Overall, I was not able to prove my hypothesis that the news subreddit had a significantly more negative bias, lending to the demoralization of the reading public. With this exploration, however, I was still able to create models with ~80% accuracy in identifying both sentiment and subreddit - an interesting next step would be to try and consolidate these models (this would have been easier had news been more negatively biased and upliftingnews positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
